# AttentionIsAllYouNeed

This writing has based on the research paper 'Attention is all you need' publised in 2017 by scholars from google brain, which has completely changed the dimension 
of Natural Language Processing(commonly known as NLP). This paper has worked in better use of attention mechanism toward the improvement of the model called as Transformer.

#Why Transformer:
Issue with attention model based on encoder and decoder:
If the length of the input increase beyond certain limit lets say for eg. 50 or 70 words, then
it does create a issue, issue how to focus and on which word to focus so that model can
a give a accurate translation, or give some kind of answer if you are trying to build
QnA  model.
So it has some limitiation beyound that it will not be able to perform as per expectation.
So the Transformer try to eliminate the existing problem of Attention based model,
and it is state of architecture. All other model like gpt, bert is based on it albiet with dozens of stacked transfomers.

